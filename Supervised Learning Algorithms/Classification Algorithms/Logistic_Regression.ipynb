{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1eCYVhQGps48rsQX2V4UQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JP109/ML-Basics/blob/main/Supervised%20Learning%20Algorithms/Classification%20Algorithms/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0-N0XFIK69z"
      },
      "source": [
        "## **1. Binomial logistic regression with:**\n",
        "1. **sklearn**\n",
        "2. **statsmodels**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67DvTXrULHGn"
      },
      "source": [
        "**1. with sklearn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTB8c-ohJ6Oi"
      },
      "source": [
        "#import packages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7aI2XiXwYrb"
      },
      "source": [
        "The array x is required to be two-dimensional. It should have one column for each input, and the number of rows should be equal to the number of observations. To make x two-dimensional, you apply .reshape() with the arguments -1 to get as many rows as needed and 1 to get one column. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdipQoFSKAX2"
      },
      "source": [
        "#Create dummy data\n",
        "x = np.arange(10).reshape(-1, 1)\n",
        "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUeFZNjewcnh",
        "outputId": "9eccc918-74b4-42ea-9638-2ac50ee9618f"
      },
      "source": [
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]]\n",
            "[0 0 0 0 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9smrH7mwvaY"
      },
      "source": [
        "x has two dimensions:\n",
        "\n",
        "1. One column for a single input\n",
        "2. Ten rows, each corresponding to one observation\n",
        "\n",
        "y is one-dimensional with ten items. Again, each item corresponds to one observation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW-uXS1IKIEj"
      },
      "source": [
        "# Create model\n",
        "model = LogisticRegression(solver='liblinear', random_state=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB1A7l69xcDV"
      },
      "source": [
        "Parameters:\n",
        "1. **penalty**: A string ('l2' by default) that decides whether there is regularization and which approach to use. Other options are 'l1', 'elasticnet', and 'none'.\n",
        "\n",
        "2. **dual**: A Boolean (False by default) that decides whether to use primal (when False) or dual formulation (when True).\n",
        "\n",
        "3. **tol**: A floating-point number (0.0001 by default) that defines the tolerance for stopping the procedure.\n",
        "\n",
        "4. **C**: A positive floating-point number (1.0 by default) that defines the relative strength of regularization. Smaller values indicate stronger regularization.\n",
        "\n",
        "5. **fit_intercept**: A Boolean (True by default) that decides whether to calculate the intercept ùëè‚ÇÄ (when True) or consider it equal to zero (when False).\n",
        "\n",
        "6. **intercept_scaling**: floating-point number (1.0 by default) that defines the scaling of the intercept ùëè‚ÇÄ.\n",
        "\n",
        "7. **class_weight**: A dictionary, 'balanced', or None (default) that defines the weights related to each class. When None, all classes have the weight one.\n",
        "\n",
        "8. **random_state** is an integer, an instance of numpy.RandomState, or None (default) that defines what pseudo-random number generator to use.\n",
        "\n",
        "9. **solver** is a string ('liblinear' by default) that decides what solver to use for fitting the model. Other options are 'newton-cg', 'lbfgs', 'sag', and 'saga'.\n",
        "\n",
        "10. **max_iter** is an integer (100 by default) that defines the maximum number of iterations by the solver during model fitting.\n",
        "\n",
        "11. **multi_class** is a string ('ovr' by default) that decides the approach to use for handling multiple classes. Other options are 'multinomial' and 'auto'.\n",
        "\n",
        "12. **verbose** is a non-negative integer (0 by default) that defines the verbosity for the 'liblinear' and 'lbfgs' solvers.\n",
        "\n",
        "13. **warm_start** is a Boolean (False by default) that decides whether to reuse the previously obtained solution.\n",
        "\n",
        "14. **n_jobs** is an integer or None (default) that defines the number of parallel processes to use. None usually means to use one core, while -1 means to use all available cores.\n",
        "\n",
        "15. **l1_ratio** is either a floating-point number between zero and one or None (default). It defines the relative importance of the L1 part in the elastic-net regularization.\n",
        "\n",
        "You should carefully match the solver and regularization method for several reasons:\n",
        "\n",
        "'liblinear' solver doesn‚Äôt work without regularization.\n",
        "'newton-cg', 'sag', 'saga', and 'lbfgs' don‚Äôt support L1 regularization.\n",
        "'saga' is the only solver that supports elastic-net regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRReh_bWKL8Z",
        "outputId": "cc012b66-d1f6-4f0f-de9f-29246db1db5a"
      },
      "source": [
        "# Train model\n",
        "model.fit(x, y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbFgQuA3yw58"
      },
      "source": [
        "# You can use the fact that .fit() returns the model instance and chain the last two statements. They are equivalent to the following line of code:\n",
        "# model = LogisticRegression(solver='liblinear', random_state=0).fit(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn5U6YNqKWcZ",
        "outputId": "7527973b-903e-496a-d3fb-e08653c8b84f"
      },
      "source": [
        "# Model attributes\n",
        "print(model.classes_)\n",
        "print(model.intercept_)\n",
        "print(model.coef_)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n",
            "[-1.04608067]\n",
            "[[0.51491375]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BANfmXsRKfCn",
        "outputId": "89de6400-1319-404b-ee23-5e98595c5da1"
      },
      "source": [
        "# Model evaluation\n",
        "model.predict_proba(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.74002157, 0.25997843],\n",
              "       [0.62975524, 0.37024476],\n",
              "       [0.5040632 , 0.4959368 ],\n",
              "       [0.37785549, 0.62214451],\n",
              "       [0.26628093, 0.73371907],\n",
              "       [0.17821501, 0.82178499],\n",
              "       [0.11472079, 0.88527921],\n",
              "       [0.07186982, 0.92813018],\n",
              "       [0.04422513, 0.95577487],\n",
              "       [0.02690569, 0.97309431]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnq9TvNZKjrx",
        "outputId": "2f23bc62-8d68-4e0d-b5cc-70e2c5c74665"
      },
      "source": [
        "model.predict(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDjBbC8CKn3H",
        "outputId": "aec4b3ca-9df5-41f6-8eb3-f7245ab19265"
      },
      "source": [
        "model.score(x, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T0_oVFYKz2X",
        "outputId": "39dc4d24-7347-4e67-ff3a-850a55ce87a4"
      },
      "source": [
        "# Example of logistic regression model with regulariztion strength = 10\n",
        "model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\n",
        "model.fit(x, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXRMLPyPLLos"
      },
      "source": [
        "**2. with statsmodels**: When you want detailed statistics related to the model/results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woI39BhELYYH",
        "outputId": "da8c5908-06a1-49b1-f71b-930c043a5232"
      },
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "import statsmodels.api as sm"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7qjA8JN0kFY"
      },
      "source": [
        "StatsModels doesn‚Äôt take the intercept ùëè‚ÇÄ into account, and you need to include the additional column of ones in x. You do that with add_constant():\n",
        "\n",
        "add_constant() takes the array x as the argument and returns a new array with the additional column of ones. This is how x and y look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdr0Y805LjPF"
      },
      "source": [
        "# Create dummy data\n",
        "x = np.arange(10).reshape(-1, 1)\n",
        "y = np.array([0, 1, 0, 0, 1, 1, 1, 1, 1, 1])\n",
        "x = sm.add_constant(x)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IsMJ0jL0sxB",
        "outputId": "64c1d8f8-d301-4ae7-d538-09bd66b7f842"
      },
      "source": [
        "x"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [1., 1.],\n",
              "       [1., 2.],\n",
              "       [1., 3.],\n",
              "       [1., 4.],\n",
              "       [1., 5.],\n",
              "       [1., 6.],\n",
              "       [1., 7.],\n",
              "       [1., 8.],\n",
              "       [1., 9.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk4JPqZX0yIy"
      },
      "source": [
        "The first column of x corresponds to the intercept ùëè‚ÇÄ. The second column contains the original values of x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97Z_PBKfLnUa"
      },
      "source": [
        "# Create model\n",
        "model = sm.Logit(y, x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_AD3_PoLqaL",
        "outputId": "d6f5b917-22e8-442f-8757-b2e83ef4888c"
      },
      "source": [
        "# Train model\n",
        "result = model.fit(method='newton')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.350471\n",
            "         Iterations 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVUajOtLLtQT",
        "outputId": "3646d5fd-541e-43a1-a4bb-b0962fec6293"
      },
      "source": [
        "# b0 and b1 values (intercept and slope respectively)\n",
        "result.params"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.972805  ,  0.82240094])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOTUV1_2LxNy",
        "outputId": "0f66279b-c638-4cda-dcef-b81710a163f0"
      },
      "source": [
        "# Predict here gives probabilities rather than predictions, given by scikit learns predict\n",
        "result.predict(x)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.12208792, 0.24041529, 0.41872657, 0.62114189, 0.78864861,\n",
              "       0.89465521, 0.95080891, 0.97777369, 0.99011108, 0.99563083])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGorfLBZLzds",
        "outputId": "09ea692f-eade-4f0a-d9e6-a8f50a7fd213"
      },
      "source": [
        "# Get actual predictions\n",
        "(result.predict(x) >= 0.5).astype(int)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YHKCI6vL2US",
        "outputId": "cb57672d-9227-4240-9951-1b52df9fb893"
      },
      "source": [
        "# Confusion matrix here has floats instead of integers\n",
        "result.pred_table()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2., 1.],\n",
              "       [1., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "AXWsNOTiL-sQ",
        "outputId": "517cc8d5-ecf8-4e67-bac9-359790a23ce3"
      },
      "source": [
        "result.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Logit Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>    10</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>     8</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>            <td>Sun, 10 Oct 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.4263</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                <td>15:34:41</td>     <th>  Log-Likelihood:    </th> <td> -3.5047</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -6.1086</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.02248</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>const</th> <td>   -1.9728</td> <td>    1.737</td> <td>   -1.136</td> <td> 0.256</td> <td>   -5.377</td> <td>    1.431</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x1</th>    <td>    0.8224</td> <td>    0.528</td> <td>    1.557</td> <td> 0.119</td> <td>   -0.213</td> <td>    1.858</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                           Logit Regression Results                           \n",
              "==============================================================================\n",
              "Dep. Variable:                      y   No. Observations:                   10\n",
              "Model:                          Logit   Df Residuals:                        8\n",
              "Method:                           MLE   Df Model:                            1\n",
              "Date:                Sun, 10 Oct 2021   Pseudo R-squ.:                  0.4263\n",
              "Time:                        15:34:41   Log-Likelihood:                -3.5047\n",
              "converged:                       True   LL-Null:                       -6.1086\n",
              "Covariance Type:            nonrobust   LLR p-value:                   0.02248\n",
              "==============================================================================\n",
              "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "const         -1.9728      1.737     -1.136      0.256      -5.377       1.431\n",
              "x1             0.8224      0.528      1.557      0.119      -0.213       1.858\n",
              "==============================================================================\n",
              "\"\"\""
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "2eovO-RgMAYm",
        "outputId": "475bf0cc-b723-4762-e6c0-b4d4c0461c56"
      },
      "source": [
        "result.summary2()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>   <td>0.426</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <td>Dependent Variable:</td>         <td>y</td>              <td>AIC:</td>         <td>11.0094</td>\n",
              "</tr>\n",
              "<tr>\n",
              "         <td>Date:</td>        <td>2021-10-10 15:34</td>       <td>BIC:</td>         <td>11.6146</td>\n",
              "</tr>\n",
              "<tr>\n",
              "   <td>No. Observations:</td>         <td>10</td>         <td>Log-Likelihood:</td>   <td>-3.5047</td>\n",
              "</tr>\n",
              "<tr>\n",
              "       <td>Df Model:</td>              <td>1</td>            <td>LL-Null:</td>       <td>-6.1086</td>\n",
              "</tr>\n",
              "<tr>\n",
              "     <td>Df Residuals:</td>            <td>8</td>          <td>LLR p-value:</td>    <td>0.022485</td>\n",
              "</tr>\n",
              "<tr>\n",
              "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>        <td>1.0000</td> \n",
              "</tr>\n",
              "<tr>\n",
              "    <td>No. Iterations:</td>        <td>7.0000</td>              <td></td>              <td></td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "    <td></td>     <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>const</th> <td>-1.9728</td>  <td>1.7366</td>  <td>-1.1360</td> <td>0.2560</td> <td>-5.3765</td> <td>1.4309</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x1</th>    <td>0.8224</td>   <td>0.5281</td>  <td>1.5572</td>  <td>0.1194</td> <td>-0.2127</td> <td>1.8575</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary2.Summary'>\n",
              "\"\"\"\n",
              "                        Results: Logit\n",
              "===============================================================\n",
              "Model:              Logit            Pseudo R-squared: 0.426   \n",
              "Dependent Variable: y                AIC:              11.0094 \n",
              "Date:               2021-10-10 15:34 BIC:              11.6146 \n",
              "No. Observations:   10               Log-Likelihood:   -3.5047 \n",
              "Df Model:           1                LL-Null:          -6.1086 \n",
              "Df Residuals:       8                LLR p-value:      0.022485\n",
              "Converged:          1.0000           Scale:            1.0000  \n",
              "No. Iterations:     7.0000                                     \n",
              "-----------------------------------------------------------------\n",
              "          Coef.    Std.Err.      z      P>|z|     [0.025   0.975]\n",
              "-----------------------------------------------------------------\n",
              "const    -1.9728     1.7366   -1.1360   0.2560   -5.3765   1.4309\n",
              "x1        0.8224     0.5281    1.5572   0.1194   -0.2127   1.8575\n",
              "===============================================================\n",
              "\n",
              "\"\"\""
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc6jPFKPMQUT"
      },
      "source": [
        "## **2. Multinomial Logistic Regression**\n",
        "\n",
        "One popular approach for adapting logistic regression to multi-class classification problems is to split the multi-class classification problem into multiple binary classification problems and fit a standard logistic regression model on each subproblem. Techniques of this type include one-vs-rest and one-vs-one wrapper models.\n",
        "\n",
        "An alternate approach involves changing the logistic regression model to support the prediction of multiple class labels directly. Specifically, to predict the probability that an input example belongs to each known class label.Changing logistic regression from binomial to multinomial probability requires a change to the loss function used to train the model (e.g. log loss to cross-entropy loss), and a change to the output from a single probability value to one probability for each class label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otSsRA_iMUQl",
        "outputId": "3a9953db-30e8-4c03-cb20-0a309280447d"
      },
      "source": [
        "# test classification dataset\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)\n",
        "print(Counter(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 10) (1000,)\n",
            "Counter({1: 334, 2: 334, 0: 332})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF1vb8pA9KOk"
      },
      "source": [
        "The LogisticRegression class can be configured for multinomial logistic regression by setting the ‚Äúmulti_class‚Äù argument to ‚Äúmultinomial‚Äù and the ‚Äúsolver‚Äù argument to a solver that supports multinomial logistic regression, such as ‚Äúlbfgs‚Äú"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7JZOrCvM7rh"
      },
      "source": [
        "# define the multinomial logistic regression model\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoi-tPpe9UZP"
      },
      "source": [
        "It is a good practice to evaluate classification models using repeated stratified k-fold cross-validation. The stratification ensures that each cross-validation fold has approximately the same distribution of examples in each class as the whole training dataset.\n",
        "\n",
        "We will use three repeats with 10 folds, which is a good default, and evaluate model performance using classification accuracy given that the classes are balanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnCtT1u8NBTw",
        "outputId": "faa890e6-fdad-4e0e-f4be-43e0b0744ff1"
      },
      "source": [
        "# evaluate multinomial logistic regression model\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
        "# define the multinomial logistic regression model\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "# define the model evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate the model and collect the scores\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report the model performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.681 (0.042)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34tPDOKMNEkh",
        "outputId": "56398502-2ef3-4ca6-bc07-7bf051aef912"
      },
      "source": [
        "# make a prediction with a multinomial logistic regression model, on new data\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
        "# define the multinomial logistic regression model\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "# fit the model on the whole dataset\n",
        "model.fit(X, y)\n",
        "# define a single row of input data\n",
        "row = [1.89149379, -0.39847585, 1.63856893, 0.01647165, 1.51892395, -3.52651223, 1.80998823, 0.58810926, -0.02542177, -0.52835426]\n",
        "# predict the class label\n",
        "yhat = model.predict([row])\n",
        "# summarize the predicted class\n",
        "print('Predicted Class: %d' % yhat[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k16LFhJH-2hu"
      },
      "source": [
        "A benefit of multinomial logistic regression is that it can predict calibrated probabilities across all known class labels in the dataset.\n",
        "\n",
        "This can be achieved by calling the predict_proba() function on the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw1uEX9xNIl6",
        "outputId": "d4671bd0-d21c-49cf-d095-84ddbf88ab0e"
      },
      "source": [
        "# make a prediction with a multinomial logistic regression model\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
        "# define the multinomial logistic regression model\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "# fit the model on the whole dataset\n",
        "model.fit(X, y)\n",
        "# define a single row of input data\n",
        "row = [1.89149379, -0.39847585, 1.63856893, 0.01647165, 1.51892395, -3.52651223, 1.80998823, 0.58810926, -0.02542177, -0.52835426]\n",
        "# predict the class label\n",
        "yhat = model.predict([row])\n",
        "# summarize the predicted class\n",
        "print('Predicted Class: %d' % yhat[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mahcA_jR_aTG"
      },
      "source": [
        "**Tuning multinomial logistic regression**\n",
        "\n",
        "By default, the LogisticRegression class uses the L2 penalty with a weighting of coefficients set to 1.0. The type of penalty can be set via the ‚Äúpenalty‚Äù argument with values of ‚Äúl1‚Äú, ‚Äúl2‚Äú, ‚Äúelasticnet‚Äù (e.g. both), although not all solvers support all penalty types. The weighting of the coefficients in the penalty can be set via the ‚ÄúC‚Äù argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLcCbhXSNQZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e86611-3a1b-4691-c7dd-24ffd5ed176b"
      },
      "source": [
        "# define the multinomial logistic regression model with a default penalty\n",
        "LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=1.0)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLktJNsf_zff"
      },
      "source": [
        "The weighting for the penalty is actually the inverse weighting, perhaps penalty = 1 ‚Äì C.\n",
        "\n",
        "This means that values close to 1.0 indicate very little penalty and values close to zero indicate a strong penalty. A C value of 1.0 may indicate no penalty at all.\n",
        "\n",
        "*   C close to 1.0: Light penalty.\n",
        "*   C close to 0.0: Strong penalty.\n",
        "\n",
        "The penalty can be disabled by setting the ‚Äúpenalty‚Äù argument to the string ‚Äúnone‚Äú."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuo4-3Vx_0Rh",
        "outputId": "790a43c0-90cb-433b-d78d-94196e1e017e"
      },
      "source": [
        "# define the multinomial logistic regression model without a penalty\n",
        "LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='none',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0GcDqhfAQNw"
      },
      "source": [
        "It is common to test penalty values on a log scale in order to quickly discover the scale of penalty that works well for a model. Once found, further tuning at that scale may be beneficial.\n",
        "\n",
        "We will explore the L2 penalty with weighting values in the range from 0.0001 to 1.0 on a log scale, in addition to no penalty or 0.0.\n",
        "\n",
        "The complete example of evaluating L2 penalty values for multinomial logistic regression is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "dGVn3UtTAQ8U",
        "outputId": "ca4989be-2f18-4fa9-a374-754f49c238bf"
      },
      "source": [
        "# tune regularization for multinomial logistic regression\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from matplotlib import pyplot\n",
        " \n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
        "\treturn X, y\n",
        " \n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
        "\t\t# create name for model\n",
        "\t\tkey = '%.4f' % p\n",
        "\t\t# turn off penalty in some cases\n",
        "\t\tif p == 0.0:\n",
        "\t\t\t# no penalty in this case\n",
        "\t\t\tmodels[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
        "\t\telse:\n",
        "\t\t\tmodels[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
        "\treturn models\n",
        " \n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "\t# define the evaluation procedure\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\t# evaluate the model\n",
        "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\treturn scores\n",
        " \n",
        "# define dataset\n",
        "X, y = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\t# evaluate the model and collect the scores\n",
        "\tscores = evaluate_model(model, X, y)\n",
        "\t# store the results\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\t# summarize progress along the way\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">0.0000 0.777 (0.037)\n",
            ">0.0001 0.683 (0.049)\n",
            ">0.0010 0.762 (0.044)\n",
            ">0.0100 0.775 (0.040)\n",
            ">0.1000 0.774 (0.038)\n",
            ">1.0000 0.777 (0.037)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3QcfmbRAjA3"
      },
      "source": [
        "In this case, we can see that a C value of 1.0 has the best score of about 77.7 percent, which is the same as using no penalty that achieves the same score.\n",
        "\n",
        "A box and whisker plot is created for the accuracy scores for each configuration and all plots are shown side by side on a figure on the same scale for direct comparison.\n",
        "\n",
        "In this case, we can see that the larger penalty we use on this dataset (i.e. the smaller the C value), the worse the performance of the model."
      ]
    }
  ]
}